{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnKbaTRDefA",
        "outputId": "6954b567-cc0b-407c-db1a-277f17414c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla V100-SXM2-16GB\n",
            "---------SFLV1 ResNet18 on Covid-19----------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os.path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from pandas import DataFrame\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n",
        "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omc60-QKJPFA"
      },
      "outputs": [],
      "source": [
        "# No. of users\n",
        "num_users = 2\n",
        "epochs = 50\n",
        "frac = 1\n",
        "lr = 0.0001\n",
        "num_classes = 3\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZD4RBruDgit"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_users = 2\n",
        "epochs = 200\n",
        "frac = 1        \n",
        "lr = 0.0001\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9i9rXVDj-p",
        "outputId": "b8457e0f-8413-4983-f7c5-3dce6cfe739a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_nt-h2-DqcT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "data_path = '/content/drive/My Drive/case4'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=data_path + '/train1',\n",
        "    transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(), \n",
        "    transforms.RandomRotation(30),  \n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), \n",
        "    transforms.RandomGrayscale(p=0.1),  \n",
        "    transforms.RandomPerspective(),  \n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=data_path + '/test1',\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CCrAXHbDxM-",
        "outputId": "16e2dae4-bb4d-46cc-b2b1-e44e7f8d344a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples under each class:\n",
            "MildDemented: 386\n",
            "ModerateDemented: 282\n",
            "NonDemented: 640\n",
            "Number of samples under each class:\n",
            "MildDemented: 100\n",
            "ModerateDemented: 59\n",
            "NonDemented: 160\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class_counts = {}\n",
        "for class_idx, class_name in enumerate(train_dataset.classes):\n",
        "    class_counts[class_name] = len([label for _, label in train_dataset.samples if label == class_idx])\n",
        "\n",
        "print(\"Number of samples under each class:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "class_counts = {}\n",
        "for class_idx, class_name in enumerate(test_dataset.classes):\n",
        "    class_counts[class_name] = len([label for _, label in test_dataset.samples if label == class_idx])\n",
        "\n",
        "print(\"Number of samples under each class:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN64WJiRD6qZ",
        "outputId": "fae82e84-9607-48a4-987c-cbead4619be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet18_client_side(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#=====================================================================================================\n",
        "#                           Client-side Model definition\n",
        "#=====================================================================================================\n",
        "\n",
        "class ResNet18_client_side(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18_client_side, self).__init__()\n",
        "        self.layer1 = nn.Sequential (\n",
        "                nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU (inplace = True),\n",
        "                nn.MaxPool2d(kernel_size = 3, stride = 2, padding =1),\n",
        "            )\n",
        "        self.layer2 = nn.Sequential  (\n",
        "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU (inplace = True),\n",
        "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
        "                nn.BatchNorm2d(64),\n",
        "            )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        resudial1 = F.relu(self.layer1(x))\n",
        "        out1 = self.layer2(resudial1)\n",
        "        out1 = out1 + resudial1\n",
        "        resudial2 = F.relu(out1)\n",
        "        return resudial2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net_glob_client = ResNet18_client_side()\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
        "    net_glob_client = nn.DataParallel(net_glob_client)\n",
        "\n",
        "net_glob_client.to(device)\n",
        "print(net_glob_client)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o2wdU30EnCC",
        "outputId": "e47a4ae7-c392-4dfd-ccae-7f5c349a8314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet18_server_side(\n",
            "  (layer3): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Baseblock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dim_change): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Baseblock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer5): Sequential(\n",
            "    (0): Baseblock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dim_change): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Baseblock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer6): Sequential(\n",
            "    (0): Baseblock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dim_change): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Baseblock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (averagePool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#%%%\n",
        "#=====================================================================================================\n",
        "#                           Server-side Model definition\n",
        "#=====================================================================================================\n",
        "\n",
        "\n",
        "num_classes =  3\n",
        "\n",
        "\n",
        "class Baseblock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, input_planes, planes, stride=1, dim_change=None):\n",
        "        super(Baseblock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_planes, planes, stride=stride, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, stride=1, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dim_change = dim_change\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        output = F.relu(self.bn1(self.conv1(x)))\n",
        "        output = self.bn2(self.conv2(output))\n",
        "\n",
        "        if self.dim_change is not None:\n",
        "            res = self.dim_change(res)\n",
        "\n",
        "        output += res\n",
        "        output = F.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResNet18_server_side(nn.Module):\n",
        "    def __init__(self, block, num_layers, num_classes):\n",
        "        super(ResNet18_server_side, self).__init__()\n",
        "        self.input_planes = 64\n",
        "\n",
        "        # Modify the first layer to accept the correct number of input channels\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "\n",
        "        self.layer4 = self._layer(block, 128, num_layers[0], stride=2)\n",
        "        self.layer5 = self._layer(block, 256, num_layers[1], stride=2)\n",
        "        self.layer6 = self._layer(block, 512, num_layers[2], stride=2)\n",
        "        self.averagePool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _layer(self, block, planes, num_layers, stride=2):\n",
        "        dim_change = None\n",
        "        if stride != 1 or planes != self.input_planes * block.expansion:\n",
        "            dim_change = nn.Sequential(\n",
        "                nn.Conv2d(self.input_planes, planes * block.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion)\n",
        "            )\n",
        "        netLayers = [block(self.input_planes, planes, stride=stride, dim_change=dim_change)]\n",
        "        self.input_planes = planes * block.expansion\n",
        "        for _ in range(1, num_layers):\n",
        "            netLayers.append(block(self.input_planes, planes))\n",
        "            self.input_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*netLayers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out2 = self.layer3(x)\n",
        "        out2 = out2 + x  # adding the residual inputs -- downsampling not required in this layer\n",
        "        x3 = F.relu(out2)\n",
        "\n",
        "        x4 = self.layer4(x3)\n",
        "        x5 = self.layer5(x4)\n",
        "        x6 = self.layer6(x5)\n",
        "\n",
        "        x7 = F.avg_pool2d(x6, 7)\n",
        "        x8 = x7.view(x7.size(0), -1)\n",
        "        y_hat = self.fc(x8)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "# Instantiate the server-side model\n",
        "net_glob_server = ResNet18_server_side(Baseblock, [2, 2, 2], num_classes)\n",
        "\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n",
        "    net_glob_server = nn.DataParallel(net_glob_server)\n",
        "\n",
        "net_glob_server.to(device)\n",
        "\n",
        "# Display the model architecture\n",
        "print(net_glob_server)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AqyLtKjD6uQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====================================================================================\n",
        "# For Server Side Loss and Accuracy\n",
        "loss_train_collect = []\n",
        "acc_train_collect = []\n",
        "loss_test_collect = []\n",
        "acc_test_collect = []\n",
        "batch_acc_train = []\n",
        "batch_loss_train = []\n",
        "batch_acc_test = []\n",
        "batch_loss_test = []\n",
        "\n",
        "num_classes= 3 \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhOPguijD6yE"
      },
      "outputs": [],
      "source": [
        "#%%%\n",
        "# ====================================================================================================\n",
        "#                               Server Side Program\n",
        "# ====================================================================================================\n",
        "num_classes=3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Federated averaging: FedAvg\n",
        "def FedAvg(w):\n",
        "    w_avg = copy.deepcopy(w[0])\n",
        "    for k in w_avg.keys():\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[k] += w[i][k]\n",
        "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
        "    return w_avg\n",
        "\n",
        "\n",
        "def calculate_accuracy(fx, y):\n",
        "    preds = fx.max(1, keepdim=True)[1]\n",
        "    correct = preds.eq(y.view_as(preds)).sum()\n",
        "    acc = 100.00 * correct.float() / preds.shape[0]\n",
        "    return acc\n",
        "\n",
        "def calculate_precision(fx, y):\n",
        "    preds = fx.max(1, keepdim=True)[1]\n",
        "\n",
        "    true_positives = (preds.eq(y.view_as(preds)) & (y == 1)).sum().item()\n",
        "    false_positives = (preds.ne(y.view_as(preds)) & (y == 0)).sum().item()\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
        "\n",
        "    return precision\n",
        "\n",
        "def calculate_recall(fx, y):\n",
        "    preds = fx.max(1, keepdim=True)[1]\n",
        "\n",
        "    true_positives = (preds.eq(y.view_as(preds)) & (y == 1)).sum().item()\n",
        "    false_negatives = (preds.ne(y.view_as(preds)) & (y == 1)).sum().item()\n",
        "\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
        "\n",
        "    return recall\n",
        "\n",
        "\n",
        "acc_avg_all_user_train = 0\n",
        "loss_avg_all_user_train = 0\n",
        "loss_train_collect_user = []\n",
        "acc_train_collect_user = []\n",
        "loss_test_collect_user = []\n",
        "acc_test_collect_user = []\n",
        "\n",
        "w_glob_server = net_glob_server.state_dict()\n",
        "w_locals_server = []\n",
        "\n",
        "# client idx collector\n",
        "idx_collect = []\n",
        "l_epoch_check = False\n",
        "fed_check = False\n",
        "\n",
        "net_model_server = [ResNet18_server_side(Baseblock, [2, 2, 2, 2], num_classes) for i in range(num_users)]\n",
        "\n",
        "net_server = copy.deepcopy(net_model_server[0]).to(device)\n",
        "optimizer_server = torch.optim.Adam(net_server.parameters(), lr=lr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0X5ndtySgQR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtB6lLajD61X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Server-side function associated with Training\n",
        "def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n",
        "    global net_model_server, criterion, optimizer_server, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n",
        "    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect, w_locals_server, w_glob_server, net_server\n",
        "    global loss_train_collect_user, acc_train_collect_user, lr\n",
        "\n",
        "    net_server = copy.deepcopy(net_model_server[idx]).to(device)\n",
        "    net_server.train()\n",
        "    optimizer_server = torch.optim.Adam(net_server.parameters(), lr=lr)\n",
        "\n",
        "    # Train and update\n",
        "    optimizer_server.zero_grad()\n",
        "\n",
        "    fx_client = fx_client.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Forward prop\n",
        "    fx_server = net_server(fx_client)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(fx_server, y)\n",
        "    # Calculate accuracy\n",
        "    acc = calculate_accuracy(fx_server, y)\n",
        "    # Calculate precision\n",
        "    precision = calculate_precision(fx_server, y)\n",
        "    # Calculate recall\n",
        "    recall = calculate_recall(fx_server, y)\n",
        "    # Backward prop\n",
        "    loss.backward()\n",
        "    dfx_client = fx_client.grad.clone().detach()\n",
        "    optimizer_server.step()\n",
        "\n",
        "    batch_loss_train.append(loss.item())\n",
        "    batch_acc_train.append(acc.item())\n",
        "\n",
        "    net_model_server[idx] = copy.deepcopy(net_server)\n",
        "\n",
        "    count1 += 1\n",
        "    if count1 == len_batch:\n",
        "        acc_avg_train = sum(batch_acc_train)/len(batch_acc_train)  \n",
        "        loss_avg_train = sum(batch_loss_train)/len(batch_loss_train)\n",
        "\n",
        "        batch_acc_train = []\n",
        "        batch_loss_train = []\n",
        "        count1 = 0\n",
        "\n",
        "        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f} \\tPrecision: {:.3f} \\tRecall: {:.3f} '.format(idx, l_epoch_count, acc_avg_train, loss_avg_train, precision, recall))\n",
        "\n",
        "        w_server = net_server.state_dict()\n",
        "\n",
        "        if l_epoch_count == l_epoch-1:\n",
        "\n",
        "            l_epoch_check = True \n",
        "            w_locals_server.append(copy.deepcopy(w_server))\n",
        "\n",
        "\n",
        "            acc_avg_train_all = acc_avg_train\n",
        "            loss_avg_train_all = loss_avg_train\n",
        "\n",
        "            loss_train_collect_user.append(loss_avg_train_all)\n",
        "            acc_train_collect_user.append(acc_avg_train_all)\n",
        "\n",
        "            if idx not in idx_collect:\n",
        "                idx_collect.append(idx)\n",
        "\n",
        "        if len(idx_collect) == num_users:\n",
        "            fed_check = True  \n",
        "            w_glob_server = FedAvg(w_locals_server)\n",
        "\n",
        "            # Server-side global model update and distribute that model to all clients\n",
        "            net_glob_server.load_state_dict(w_glob_server)\n",
        "            net_model_server = [net_glob_server for i in range(num_users)]\n",
        "\n",
        "            w_locals_server = []\n",
        "            idx_collect = []\n",
        "\n",
        "            acc_avg_all_user_train = sum(acc_train_collect_user)/len(acc_train_collect_user)\n",
        "            loss_avg_all_user_train = sum(loss_train_collect_user)/len(loss_train_collect_user)\n",
        "\n",
        "            loss_train_collect.append(loss_avg_all_user_train)\n",
        "            acc_train_collect.append(acc_avg_all_user_train)\n",
        "\n",
        "            acc_train_collect_user = []\n",
        "            loss_train_collect_user = []\n",
        "\n",
        "    # Send gradients to the client\n",
        "    return dfx_client\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W20zBMOJEQio"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Server-side functions associated with Testing\n",
        "def evaluate_server(fx_client, y, idx, len_batch, ell):\n",
        "    global net_model_server, criterion, batch_acc_test, batch_loss_test, check_fed, net_server, net_glob_server\n",
        "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, w_glob_server, l_epoch_check, fed_check\n",
        "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
        "    batch_prec_test = []\n",
        "    batch_recall_test = []\n",
        "    net = copy.deepcopy(net_model_server[idx]).to(device)\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fx_client = fx_client.to(device)\n",
        "        y = y.to(device)\n",
        "        #---------forward prop-------------\n",
        "        fx_server = net(fx_client)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(fx_server, y)\n",
        "        # Calculate accuracy\n",
        "        acc = calculate_accuracy(fx_server, y)\n",
        "\n",
        "        y_true = y.cpu().numpy()\n",
        "        y_pred = fx_server.cpu().max(1, keepdim=True)[1].numpy().flatten()\n",
        "        y_score = F.softmax(fx_server, dim=1)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        batch_loss_test.append(loss.item())\n",
        "        batch_acc_test.append(acc.item())\n",
        "        batch_prec_test.append(precision)\n",
        "        batch_recall_test.append(recall)\n",
        "\n",
        "        count2 += 1\n",
        "        if count2 == len_batch:\n",
        "            acc_avg_test = sum(batch_acc_test)/len(batch_acc_test)\n",
        "            loss_avg_test = sum(batch_loss_test)/len(batch_loss_test)\n",
        "             # Check if any batches were processed\n",
        "            if len(batch_prec_test) > 0:\n",
        "              prec_avg_test = sum(batch_prec_test) / len(batch_prec_test)\n",
        "            else:\n",
        "              prec_avg_test = 0  \n",
        "\n",
        "            if len(batch_recall_test) > 0:\n",
        "              recall_avg_test = sum(batch_recall_test) / len(batch_recall_test)\n",
        "            else:\n",
        "              recall_avg_test = 0  \n",
        "\n",
        "            batch_acc_test = []\n",
        "            batch_loss_test = []\n",
        "            batch_prec_test = []\n",
        "            batch_recall_test = []\n",
        "            count2 = 0\n",
        "\n",
        "            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f} \\tPrecision: {:.3f} \\tRecall: {:.3f} '.format(idx, acc_avg_test, loss_avg_test, prec_avg_test, recall_avg_test))\n",
        "\n",
        "            if l_epoch_check:\n",
        "                l_epoch_check = False\n",
        "\n",
        "                acc_avg_test_all = acc_avg_test\n",
        "                loss_avg_test_all = loss_avg_test\n",
        "\n",
        "                loss_test_collect_user.append(loss_avg_test_all)\n",
        "                acc_test_collect_user.append(acc_avg_test_all)\n",
        "\n",
        "            if fed_check:\n",
        "                fed_check = False\n",
        "                print(\"------------------------------------------------\")\n",
        "                print(\"------ Federation process at Server-Side ------- \")\n",
        "                print(\"------------------------------------------------\")\n",
        "\n",
        "                acc_avg_all_user = sum(acc_test_collect_user)/len(acc_test_collect_user)\n",
        "                loss_avg_all_user = sum(loss_test_collect_user)/len(loss_test_collect_user)\n",
        "\n",
        "                loss_test_collect.append(loss_avg_all_user)\n",
        "                acc_test_collect.append(acc_avg_all_user)\n",
        "                acc_test_collect_user = []\n",
        "                loss_test_collect_user= []\n",
        "\n",
        "                print(\"====================== SERVER V1==========================\")\n",
        "                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train, loss_avg_all_user_train))\n",
        "                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user, loss_avg_all_user))\n",
        "                print(\"==========================================================\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG1ZbnPVPzFj"
      },
      "outputs": [],
      "source": [
        "batch_prec_test = []\n",
        "batch_recall_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SlemMIWEQlr"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def evaluate_accuracy(net, loader, device, return_conf_matrix=False):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    if return_conf_matrix:\n",
        "        return accuracy, conf_matrix\n",
        "    else:\n",
        "        return accuracy\n",
        "\n",
        "#==============================================================================================================\n",
        "#                                       Clients-side Program\n",
        "#==============================================================================================================\n",
        "class DatasetSplit(Dataset):\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = list(idxs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image, label\n",
        "\n",
        "class Client(object):\n",
        "    def __init__(self, net_client_model, idx, lr, device, train_loader=None, test_loader=None, idxs=None, idxs_test=None):\n",
        "        self.idx = idx\n",
        "        self.device = device\n",
        "        self.lr = lr\n",
        "        self.local_ep = 1\n",
        "        self.ldr_train = train_loader\n",
        "        self.ldr_test = test_loader\n",
        "\n",
        "    def train(self, net):\n",
        "        net.train()\n",
        "        optimizer_client = torch.optim.Adam(net.parameters(), lr=self.lr)\n",
        "\n",
        "        scheduler = StepLR(optimizer_client, step_size=30, gamma=0.1)\n",
        "\n",
        "        for iter in range(self.local_ep):\n",
        "            len_batch = len(self.ldr_train)\n",
        "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                optimizer_client.zero_grad()\n",
        "                #---------forward prop-------------\n",
        "                fx = net(images)\n",
        "                client_fx = fx.clone().detach().requires_grad_(True)\n",
        "\n",
        "                # Sending activations to server and receiving gradients from server\n",
        "                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n",
        "\n",
        "                #--------backward prop -------------\n",
        "                fx.backward(dfx)\n",
        "                optimizer_client.step()\n",
        "\n",
        "            # Step the learning rate scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "        return net.state_dict()\n",
        "\n",
        "\n",
        "    def evaluate(self, net, ell):\n",
        "        net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            len_batch = len(self.ldr_test)\n",
        "            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                #---------forward prop-------------\n",
        "                fx = net(images)\n",
        "\n",
        "                # Sending activations to server\n",
        "                evaluate_server(fx, labels, self.idx, len_batch, ell)\n",
        "\n",
        "        return\n",
        "\n",
        "#=====================================================================================================\n",
        "# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n",
        "def dataset_iid(dataset, num_users):\n",
        "\n",
        "    num_items = int(len(dataset) / num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSa25akdD2hW",
        "outputId": "73130e53-261e-435d-c800-8b9de5eabae6"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "# Initialize a list to store the time taken for each epoch\n",
        "epoch_times = []\n",
        "\n",
        "dict_users = dataset_iid(train_dataset, num_users)\n",
        "dict_users_test = dataset_iid(test_dataset, num_users)\n",
        "\n",
        "# Training and Testing Loop\n",
        "net_glob_client.train()\n",
        "\n",
        "# Copy weights\n",
        "w_glob_client = net_glob_client.state_dict()\n",
        "\n",
        "# Federation takes place after certain local epochs in train() client-side\n",
        "# This epoch is the global epoch, also known as rounds\n",
        "for iter in range(epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    m = max(int(frac * num_users), 1)\n",
        "    idxs_users = np.random.choice(range(num_users), m, replace=False)\n",
        "    w_locals_client = []\n",
        "\n",
        "    for idx in idxs_users:\n",
        "        local = Client(\n",
        "            net_glob_client,\n",
        "            idx,\n",
        "            lr,\n",
        "            device,\n",
        "            train_loader=train_loader,\n",
        "            test_loader=test_loader,\n",
        "            idxs=dict_users[idx],\n",
        "            idxs_test=dict_users_test[idx]\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n",
        "        w_locals_client.append(copy.deepcopy(w_client))\n",
        "\n",
        "        # Testing\n",
        "        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter)\n",
        "\n",
        "    # After serving all clients for their local epochs\n",
        "    # Federated Server: Federation process at Client-Side\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    print(\"------ FedServer: Federation process at Client-Side ------- \")\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    w_glob_client = FedAvg(w_locals_client)\n",
        "\n",
        "    # Update client-side global model\n",
        "    net_glob_client.load_state_dict(w_glob_client)\n",
        "    # Calculate epoch time\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_times.append(epoch_time)\n",
        "    print(f\"Epoch {iter + 1}/{epochs} - Time taken: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# Training and Evaluation completed!\n",
        "print(\"Training and Evaluation completed!\")\n",
        "\n",
        "# Save output data to .excel file (used for comparison plots)\n",
        "round_process = [i for i in range(1, len(acc_train_collect) + 1)]\n",
        "df = DataFrame({'round': round_process, 'acc_train': acc_train_collect, 'acc_test': acc_test_collect})\n",
        "file_path = \"/content/drive/MyDrive/Federated_Split_Learning_covid-19.xlsx\"\n",
        "df.to_excel(file_path, sheet_name=\"v1_test\", index=False)\n",
        "\n",
        "# Unmount Google Drive\n",
        "drive.flush_and_unmount()\n",
        "print(f\"Excel file saved to: {file_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
